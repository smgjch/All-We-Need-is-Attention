{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: End-to-End Machine Learning Project\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook we will take a deep dive into machine learning, and investigate some of the steps involved in a prediction task - from setting up an appropriate experimental environment and loadint the initial data set, through to performing a regression analysis by applying a variety of machine learning algorithms to the data.\n",
    "\n",
    "We have not yet covered all of the techniques which we will use in this notebook, and indeed we will not cover all of them during the course of this module. However, the purpose of this notebook is to give you an overview of what a machine learning project looks like overall. In future classes you will take a look at the  application of algorithms which we will discuss in the lectures in much more detail.\n",
    "\n",
    "This particular notebook is heavily based on the second chapter of 'Hands-On Machine Learning with Scikit-Learn & Tensorflow' by A. Geron - which is an excellent resource for introducing the practicalities of machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guidelines\n",
    "\n",
    "- The structure of the code is given to you and you will need to fill in the parts corresponding to each question. \n",
    "- Do not modify/erase other parts of the code if you have not been given specific instructions to do so.\n",
    "- When you are asked to insert code, do so between the areas which begin:\n",
    "  \n",
    "  `##########################################################`\n",
    "  \n",
    "  `# TO_DO`\n",
    "  \n",
    "  `# [your code here]`\n",
    "   \n",
    "   And which end:\n",
    "   \n",
    "  `# /TO_DO\n",
    "   ##########################################################`\n",
    "\n",
    "\n",
    "- When you are asked to comment on the results you should give clear and comprehensible explanations. Write the comments in a 'Code Cell' with a sign `#` at the beginning of each row, and in the areas which begin:\n",
    "\n",
    "  `# [INSERT YOUR ANSWER HERE]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Please do not change the cell below, you will see a number of imports. All these packages are relevant for the assignment and it is important that you get used to them. You can find more information about them in the respective documentations. The most relevant for this notebook is Scikit-Learn, which is a popular machine learning library, which features various algorithms on classification, regression and clustering. Most algorithms introduced in this course will be implemented in Scikit-Learn:\n",
    "\n",
    "https://scikit-learn.org/stable/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "For the puposes of illustration in this notebook we will use a modified version of the Boston housing price data set. This version contains 506 samples, each of which consists of 11 features and 1 label of housing price values. The features are:\n",
    "\n",
    "1. CRIM: per capita crime rate by town.\n",
    "2. ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "3. INDUS: proportion of non-retail business acres per town.\n",
    "4. CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n",
    "5. NOX: nitrogen oxides concentration (parts per 10 million).\n",
    "6. RM: average number of rooms per dwelling.\n",
    "7. AGE: proportion of owner-occupied units built prior to 1940.\n",
    "8. DIS: weighted mean of distances to five Boston employment centres.\n",
    "9. RAD: index of accessibility to radial highways.\n",
    "10. TAX: full-value property-tax rate per $10,000.\n",
    "11. PTRATIO: pupil-teacher ratio by town."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we set the path for the dataset repository, you should enter your working directory here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirName = #ENTER YOUR WORKING DRIECTORY HERE#\n",
    "dirName_feats = dirName + 'housing_features.csv'\n",
    "dirName_targs = dirName + 'housing_target.csv'\n",
    "dirName_names = dirName + 'housing_names.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now load the data into a pandas dataframe using `pd.DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_housing = pd.read_csv(dirName_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brief numerical description of dataset can be viewed by the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_housing.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can take a look at the top 5 rows of the data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's create separate holders for the input and output data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.genfromtxt(dirName_feats,delimiter=',',skip_header=1)\n",
    "y=np.genfromtxt(dirName_targs,delimiter=',',skip_header=1)\n",
    "names=np.genfromtxt(dirName_names, dtype='str',skip_header=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing\n",
    "\n",
    "Now we will take a look at different features. Please note that these steps are not required in predicting Boston housing prices, but they are here just to give you some insights into the various types of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Missing Data Items_\n",
    "\n",
    "First let's synthetically randomly replace some data entries with `nan`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "X_new = X.copy()\n",
    "mask = np.random.randint(0, 2, size=X.shape).astype(np.bool)\n",
    "X_new[mask] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to replacing the missing values with the meaan value of the corresponding column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = SimpleImputer(strategy='mean')\n",
    "X_replace_with_median = imp.fit_transform(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task:__  \n",
    "Now please try other strategies (for example, replacing missing values with the median or the most frequent value).\n",
    "\n",
    "*Hint: check the parameter 'strategy' in the `sklearn.impute.SimpleImputer` module. You may also find `help(SimpleImputer)` helpful.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "# TO_DO\n",
    "#[your code here]\n",
    "\n",
    "\n",
    "# /TO_DO\n",
    "########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Training and Test Data_\n",
    "\n",
    "The dataset is generally split into two parts: the training set and the test set. We use the training set for training a model, then apply the trained model to the test set, in order to evaluate the performance of our model.\n",
    "\n",
    "First let's split the data (randomly):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Standardisation of the Data Set_\n",
    "\n",
    "For many machiner learning algorithms very different models can be learned depedning on the scale of the input data. Quite often this can give misleading results, so standardisation of a data set is a very common technique which is applied prior to model training. The standardisation process removes the mean and scales each feature column to unit variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "Now that we have transformed our data into a more useable format, we can begin the regression task.\n",
    "\n",
    "This section solves the regression problem with different methods. We also compute mean squared error (MSE) and mean absolute error (MAE) for future use.\n",
    "\n",
    "First we create a naive baseline for compariosn, and predict the housing prices using the average values of the output data of the training samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_avg = np.ones(len(y_test)) * np.mean(y_train)\n",
    "\n",
    "mse_avg = mean_squared_error(y_test, y_avg)\n",
    "mae_avg = mean_absolute_error(y_test, y_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Linear Regression_\n",
    "\n",
    "Using the Scikit-Learn `LinearRegression` function we perform linear regression, then we compute the MSE's and MAE's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_lr = lr.predict(X_test)\n",
    "\n",
    "mse_lr = mean_squared_error(y_test, y_lr)\n",
    "mae_lr = mean_absolute_error(y_test, y_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task:__  \n",
    "Now repeat this analysis, but this time using only one feature at a time as input data. Compute the MSE's and MAE's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample, n_feature = X_train.shape\n",
    "\n",
    "mse_lr_per_feature = []\n",
    "mae_lr_per_feature = []\n",
    "\n",
    "########################################################\n",
    "# TO_DO\n",
    "#[your code here]\n",
    "\n",
    "\n",
    "# /TO_DO\n",
    "########################################################\n",
    "\n",
    "errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _k Nearest Neighbor (kNN) Regression_\n",
    "\n",
    "Using the Scikit-Learn `KNeighborsRegressor` function we perform regression, setting the `n_neigbours` hyperparamter to 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 3\n",
    "neigh = KNeighborsRegressor(n_neighbors=n_neighbors)\n",
    "neigh.fit(X_train, y_train)\n",
    "\n",
    "y_neigh = neigh.predict(X_test)\n",
    "\n",
    "mse_neigh = mean_squared_error(y_test, y_neigh)\n",
    "mae_neigh = mean_absolute_error(y_test, y_neigh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task:__  \n",
    "Now repeat the analysis, but this time use different settings of the number of nearest neighbours. Compute the MSE's and MAE's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_neigh_other_num = []\n",
    "mae_neigh_other_num = []\n",
    "\n",
    "########################################################\n",
    "# TO_DO\n",
    "#[your code here]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# /TO_DO\n",
    "########################################################\n",
    "\n",
    "errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Gradient Boosting Regression_\n",
    "\n",
    "Finally, using the Scikit-learn `GradientBoostingRegressor` function we perform regression. A number of hyperparameters can be specified in the `sklearn.ensemble.GradientBoostingRegressor` module. Here we set `loss = 'quantile'` with `alpha = 0.95`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingRegressor(loss='quantile', alpha=0.95,\n",
    "                                n_estimators=250, max_depth=3,\n",
    "                                learning_rate=.1, min_samples_leaf=9,\n",
    "                                min_samples_split=9)\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "y_gb = gb.predict(X_test)\n",
    "\n",
    "mse_gb = mean_squared_error(y_test, y_gb)\n",
    "mae_gb = mean_absolute_error(y_test, y_gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task:__  \n",
    "Now repeat the analysis, try different types of loss functions as well as other differemt types of hyperparameter settings. Compare the performances with MSEs and MAEs.\n",
    "\n",
    "*Hint: you may find the documentation helpful. To view the documentation, use the following command `help(GradientBoostingRegressor)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = ['ls', 'quantile']\n",
    "alphas = np.linspace(0.7, 0.9, 5)\n",
    "\n",
    "mse_gb_other_param = np.empty([len(losses), len(alphas)])\n",
    "mae_gb_other_param = np.empty([len(losses), len(alphas)])\n",
    "\n",
    "########################################################\n",
    "# TO_DO\n",
    "#[your code here]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# /TO_DO\n",
    "########################################################\n",
    "\n",
    "mse_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Compare Performances_\n",
    "\n",
    "This section compares the predictions using different algorithms. We first create some scatter plots to visualise the differences between actual values and the predicted values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, sharex=True)\n",
    "\n",
    "mse = [mse_avg, mse_lr, mse_neigh, mse_gb]\n",
    "mae = [mae_avg, mae_lr, mae_neigh, mae_gb]\n",
    "\n",
    "titles = ['average y', 'linear regression', 'KNN', 'gradient boosting']\n",
    "predictions = [y_avg, y_lr, y_neigh, y_gb]\n",
    "\n",
    "for counter, ax in enumerate(axes.flat):\n",
    "    ax.scatter(predictions[counter], y_test)\n",
    "    ax.set_title(titles[counter])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we view the MAE and MSE of the predicted values, consolidated using `pandas.DataFrame()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = pd.DataFrame.from_dict({'MAE': [mae_avg, mae_lr, mae_neigh, mae_gb],\n",
    "                                 'MSE': [mse_avg, mse_lr, mse_neigh, mse_gb]},\n",
    "                                 orient='index', columns=titles)\n",
    "errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task:__  \n",
    "Compare the performances with your results using different parameters in KNN and Gradient Boosting.\n",
    "\n",
    "Did different hyperparameter settings in these algorithms have any effects on the accuracy of our predictions? \n",
    "\n",
    "Could you come up with a method to automatically choose the value of these hyperparameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [INSERT YOUR ANSWER HERE]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
